# -*- coding: utf-8 -*-
"""GAN training

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lR1HQIwROAfK5o71K0wk3HimATYBTS_Z
"""

# Commented out IPython magic to ensure Python compatibility.
# train_gan.py (Maximum Speed Attempt: 32x32 + Reduced Capacity + Benchmark)
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import os
from PIL import Image, UnidentifiedImageError
from torch.amp import GradScaler, autocast # Use updated AMP API

# =====================================================================
# >>> HIGHLIGHT: Enable cuDNN Benchmark for potential conv speedup <<<
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    print("cuDNN benchmark enabled.")
# =====================================================================

# --- Hyperparameters ---
dataroot = "/content/drive/MyDrive/New folder (3)" # Your dataset path

# ================== AGGRESSIVE SPEED OPTIMIZATIONS ==================
num_epochs = 200

image_size = 32 # << KEEPING 32 for speed
ngf = 32        # << KEEPING 32 for speed
ndf = 32        # << KEEPING 32 for speed

print(f"*** WARNING: Training with reduced image size ({image_size}x{image_size}) and model capacity (ngf={ngf}, ndf={ndf}) for maximum speed. Output quality/detail will be lower. ***")
# =====================================================================

workers = 2       # Usually optimal for Colab, could try 0 if desperate
batch_size = 64
nc = 3
nz = 100
lr = 0.0002
beta1 = 0.5
ngpu = 1
save_epoch_interval = 10
output_dir = "/content/drive/MyDrive/Epochs" # New output dir name

# --- Setup Code ---
# (Quieter loader, output dirs, transforms, etc., remain the same)
os.makedirs(output_dir, exist_ok=True)
os.makedirs(os.path.join(output_dir, "images"), exist_ok=True)
os.makedirs(os.path.join(output_dir, "models"), exist_ok=True)

_warned_files = set()
def robust_pil_loader(path: str) -> Image.Image:
    global _warned_files
    try:
        with open(path, "rb") as f: img = Image.open(f); return img.convert("RGB")
    except (IOError, OSError, UnidentifiedImageError, ValueError) as e:
        if path not in _warned_files:
            print(f"WARNING: Error loading {os.path.basename(path)} ({e.__class__.__name__}). Using placeholder. Will not warn again.")
            _warned_files.add(path)
        return Image.new('RGB', (image_size, image_size), (0, 0, 0))

print(f"Loading dataset from: {dataroot}")
if not os.path.exists(dataroot): print(f"!!! ERROR: Dataset directory not found: {dataroot}"); exit()

transform=transforms.Compose([
    transforms.Resize(image_size), transforms.CenterCrop(image_size),
    transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

try:
    dataset = dset.ImageFolder(root=dataroot, transform=transform, loader=robust_pil_loader)
    if len(dataset) == 0: raise ValueError("Dataset is empty after loading.")
    print(f"Successfully initialized dataset with {len(dataset)} samples.")
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,
                            num_workers=workers, pin_memory=True)
except Exception as e:
    print(f"!!! ERROR initializing dataset/dataloader: {e}"); exit()

# --- Device Setup ---
# (Remains the same - CRITICAL TO SET COLAB RUNTIME TO GPU)
if torch.cuda.is_available() and ngpu > 0:
    device = torch.device("cuda:0"); device_type = 'cuda'
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device("cpu"); device_type = 'cpu'
    print("Using CPU."); print("!!! WARNING: Training will be very slow without GPU. Change Colab Runtime Type! !!!")
    if ngpu > 0: print("Note: ngpu=1 was set, but CUDA not found.")
    ngpu = 0

# --- Model Definitions (32x32, ngf=32, ndf=32) ---
# (Model definitions remain the same as the previous "fast" version)
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1: nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1: nn.init.normal_(m.weight.data, 1.0, 0.02); nn.init.constant_(m.bias.data, 0)

class Generator(nn.Module):
    def __init__(self, ngpu_in): super(Generator, self).__init__(); self.ngpu = ngpu_in; self.main = nn.Sequential(nn.ConvTranspose2d( nz, ngf * 4, 4, 1, 0, bias=False), nn.BatchNorm2d(ngf * 4), nn.ReLU(True), nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 2), nn.ReLU(True), nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf), nn.ReLU(True), nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False), nn.Tanh())
    def forward(self, input): return self.main(input)

class Discriminator(nn.Module):
    def __init__(self, ngpu_in): super(Discriminator, self).__init__(); self.ngpu = ngpu_in; self.main = nn.Sequential(nn.Conv2d(nc, ndf, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True), nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True), nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True), nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False), nn.Sigmoid())
    def forward(self, input): return self.main(input)

# Instantiate Models, move to device, apply weights
netG = Generator(ngpu).to(device); netD = Discriminator(ngpu).to(device)
if device.type == 'cuda' and ngpu > 1: netG = nn.DataParallel(netG, list(range(ngpu))); netD = nn.DataParallel(netD, list(range(ngpu)))
netG.apply(weights_init); netD.apply(weights_init)

# --- Training Setup ---
criterion = nn.BCELoss()
fixed_noise = torch.randn(64, nz, 1, 1, device=device)
real_label = 1.; fake_label = 0.
optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))
scaler = GradScaler(device=device_type, enabled=(device_type == 'cuda'))

# --- Training Loop (AMP enabled, 50 Epochs) ---
img_list = []; G_losses = []; D_losses = []; iters = 0
print(f"Starting FASTEST Training Loop for {num_epochs} epochs ({image_size}x{image_size})...")

for epoch in range(num_epochs):
    for i, data in enumerate(dataloader, 0):

        # --- (1) Update Discriminator ---
        # (Training step logic with autocast/scaler remains the same)
        netD.zero_grad(set_to_none=True)
        real_dev = data[0].to(device)
        b_size = real_dev.size(0)
        if b_size == 0: continue
        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)
        with autocast(device_type=device_type, enabled=(device_type == 'cuda')):
            output_real = netD(real_dev).view(-1); errD_real = criterion(output_real, label)
        scaler.scale(errD_real).backward(); D_x = output_real.mean().item()
        noise = torch.randn(b_size, nz, 1, 1, device=device)
        with autocast(device_type=device_type, enabled=(device_type == 'cuda')): fake = netG(noise)
        label.fill_(fake_label)
        with autocast(device_type=device_type, enabled=(device_type == 'cuda')):
            output_fake = netD(fake.detach()).view(-1); errD_fake = criterion(output_fake, label)
        scaler.scale(errD_fake).backward(); D_G_z1 = output_fake.mean().item(); errD = errD_real + errD_fake
        scaler.step(optimizerD)

        # --- (2) Update Generator ---
        # (Training step logic with autocast/scaler remains the same)
        netG.zero_grad(set_to_none=True)
        label.fill_(real_label)
        with autocast(device_type=device_type, enabled=(device_type == 'cuda')):
            output_g = netD(fake).view(-1); errG = criterion(output_g, label)
        scaler.scale(errG).backward(); D_G_z2 = output_g.mean().item()
        scaler.step(optimizerG)

        scaler.update()

        # --- Logging & Visualization ---
        # (Log/save logic remains the same)
        if i % 100 == 0: # Log every 100 batches
            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'
#                   % (epoch+1, num_epochs, i, len(dataloader),
                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2), flush=True)
        G_losses.append(errG.item()); D_losses.append(errD.item())
        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):
            with torch.no_grad():
                with autocast(device_type=device_type, enabled=(device_type == 'cuda')): fake_viz = netG(fixed_noise).detach().cpu()
            vutils.save_image(fake_viz, f"{output_dir}/images/fake_samples_epoch_{epoch+1}_iter_{iters}.png", normalize=True)
            if epoch == num_epochs-1 and i == len(dataloader)-1: img_list.append(vutils.make_grid(fake_viz, padding=2, normalize=True))
        iters += 1

    # --- Save Checkpoint ---
    # (Checkpoint logic remains the same)
    if (epoch + 1) % save_epoch_interval == 0 or epoch == num_epochs - 1:
        save_path_g=f'{output_dir}/models/netG_epoch_{epoch+1}.pth'; save_path_d=f'{output_dir}/models/netD_epoch_{epoch+1}.pth'
        torch.save(netG.module.state_dict() if isinstance(netG, nn.DataParallel) else netG.state_dict(), save_path_g)
        torch.save(netD.module.state_dict() if isinstance(netD, nn.DataParallel) else netD.state_dict(), save_path_d)
        print(f"Saved checkpoints epoch {epoch+1}")

print(f"Training Finished ({num_epochs} Epochs).")

# --- Plotting ---
# (Plotting code remains the same)
plt.figure(figsize=(10,5)); plt.title("G&D Loss"); plt.plot(G_losses,label="G"); plt.plot(D_losses,label="D"); plt.xlabel("iterations"); plt.ylabel("Loss"); plt.legend(); plt.savefig(f"{output_dir}/loss_plot.png")
try:
    real_batch = next(iter(dataloader)); plt.figure(figsize=(15,8))
    plt.subplot(1,2,1); plt.axis("off"); plt.title("Real Images"); plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to('cpu')[:64], padding=5, normalize=True).cpu(),(1,2,0)))
    if img_list: plt.subplot(1,2,2); plt.axis("off"); plt.title(f"Fake Images (Epoch {num_epochs})"); plt.imshow(np.transpose(img_list[-1],(1,2,0)))
    else: print("No fake images stored for final plot.")
    plt.savefig(f"{output_dir}/real_vs_fake_epoch{num_epochs}.png")
except StopIteration: print("DataLoader exhausted for plotting.")
except Exception as e: print(f"Error during final plotting: {e}")
print(f"Training artifacts saved in: {output_dir}")