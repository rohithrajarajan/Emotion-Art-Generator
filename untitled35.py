# -*- coding: utf-8 -*-
"""Untitled35.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VBY4pRfE7BzLAL4F1FO3qLMVpXxdad8I
"""

!pip install deepface

import torch
import torch.nn as nn
import torchvision.utils as vutils
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import random
import os
from deepface import DeepFace
from collections import OrderedDict
import cv2

# --- Imports specific to Colab ---
from IPython.display import display, Javascript, Image as IPyImage
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import io

# from google.colab import drive
# drive.mount('/content/drive')
generator_model_path = "/content/drive/MyDrive/Epochs/models/netG_epoch_200.pth"


nz = 100
ngf = 16
nc = 3
ngpu = 1

output_image_file = "generated_colab_emotion_art.png"

class Generator(nn.Module):
    def __init__(self, ngpu_internal):
        super(Generator, self).__init__()
        self.ngpu = ngpu_internal
        self.main = nn.Sequential(
            # Layer 0, 1, 2: Input Z -> ngf*8 channels | Output size: 4x4 (assuming nz=100)
            # state size. (ngf*8) x 4 x 4
            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False), # nz -> 128
            nn.BatchNorm2d(ngf * 8), # 128
            nn.ReLU(True),
            # Layer 3, 4, 5: -> ngf*4 channels | Output size: 8x8
            # state size. (ngf*4) x 8 x 8
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), # 128 -> 64
            nn.BatchNorm2d(ngf * 4), # 64
            nn.ReLU(True),
            # Layer 6, 7, 8: -> ngf*2 channels | Output size: 16x16
            # state size. (ngf*2) x 16 x 16
            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False), # 64 -> 32
            nn.BatchNorm2d(ngf * 2), # 32
            nn.ReLU(True),
            # Layer 9: -> nc channels | Output size: 32x32
            # state size. (nc) x 32 x 32
            nn.ConvTranspose2d( ngf * 2, nc, 4, 2, 1, bias=False), # 32 -> 3 (nc)
            # Layers corresponding to main.10 (BatchNorm) and main.12 (ConvT)
            # appear to be MISSING in the checkpoint, so they are removed here.
            nn.Tanh()
            # Final state size. (nc) x 32 x 32
        )
    def forward(self, input): return self.main(input)

# --- Emotion Mapping and Latent Vector Generation (Identical) ---
emotion_map = {
    'happy': 'Happy/Joyful', 'sad': 'Sad/Melancholic', 'angry': 'Angry/Very Negative',
    'neutral': 'Neutral/Subdued', 'fear': 'Sad/Melancholic', 'disgust': 'Angry/Very Negative',
    'surprise': 'Positive/Content'
}

def map_emotion_to_latent(emotion_label, target_device, latent_dim=nz, seed_offset=0):
    print(f"Mapping emotion '{emotion_label}' to latent vector on {target_device}.")
    base_seed = 0
    if emotion_label == "Happy/Joyful":         emotion_seed = base_seed + 1000 + seed_offset
    elif emotion_label == "Positive/Content":   emotion_seed = base_seed + 2000 + seed_offset
    elif emotion_label == "Neutral/Subdued":    emotion_seed = base_seed + 3000 + seed_offset
    elif emotion_label == "Sad/Melancholic":    emotion_seed = base_seed + 4000 + seed_offset
    elif emotion_label == "Angry/Very Negative": emotion_seed = base_seed + 5000 + seed_offset
    else:
        print(f"Warning: Unmapped emotion '{emotion_label}', using default seed.")
        emotion_seed = base_seed + 6000 + seed_offset
    torch.manual_seed(emotion_seed)
    noise = torch.randn(1, latent_dim, 1, 1, device=target_device)
    print(f"Generated latent vector using seed: {emotion_seed}")
    return noise

def generate_art_from_emotion(emotion_label, netG, target_device, latent_dim=nz):
    random_seed_offset = random.randint(0, 1000)
    latent_vector = map_emotion_to_latent(emotion_label, target_device, latent_dim, seed_offset=random_seed_offset)
    print(f"Generating image with the GAN on {target_device} for emotion: {emotion_label}")
    netG.eval()
    with torch.no_grad():
        generated_image_tensor = netG(latent_vector).detach().cpu()
    print(f"Image generation complete. Tensor shape: {generated_image_tensor.shape}")
    return generated_image_tensor

def post_process_image(image_tensor):
    grid = vutils.make_grid(image_tensor, padding=0, normalize=True)
    ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(torch.uint8).numpy()
    im = Image.fromarray(ndarr)
    return im

# --- Colab Webcam Capture Functions ---
def js_to_image(js_reply):
  """
  Converts Javascript image data captured by Colab's webcam snippet
  to an OpenCV image (BGR format).
  Params:
          js_reply: Javascript object containing image data.
  Returns:
          img: OpenCV BGR image
  """
  image_bytes = b64decode(js_reply.split(',')[1])
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  img = cv2.imdecode(jpg_as_np, flags=1)
  return img

def take_photo(filename='photo.jpg', quality=0.8):
  """
  Uses Colab's Javascript interface to take a picture using the local webcam.
  """
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)

  # get photo data from js
  data = eval_js('takePhoto({})'.format(quality))
  # decode and save photo file
  img = js_to_image(data)

  # save photo disk (optional, can process directly)
  cv2.imwrite(filename, img)

  return filename, img

if __name__ == "__main__":
    # --- Device Setup ---
    if torch.cuda.is_available() and ngpu > 0:
        device = torch.device("cuda:0")
        print(f"Using GPU: {torch.cuda.get_device_name(0)} for generation.")
    else:
        if ngpu > 0: print("WARNING: CUDA (GPU) not available, forcing CPU mode.")
        device = torch.device("cpu")
        print("Using CPU for generation.")

    try:
        from google.colab import drive
        if not os.path.exists("/content/drive/MyDrive"):
             print("Mounting Google Drive...")
             drive.mount('/content/drive', force_remount=True)
             print("Google Drive mounted.")
        else:
             print("Google Drive already mounted.")
    except ImportError:
        print("Not running in Colab environment, skipping drive mount.")
    except Exception as e:
        print(f"Error mounting Google Drive: {e}")

    # --- Load GAN Generator ---
    if not os.path.exists(generator_model_path):
        print(f"!!! ERROR: Trained generator model not found at: {generator_model_path}")
        print("Please ensure Google Drive is mounted and the path is correct.")
        exit()

    netG = Generator(ngpu_internal=0).to(device)
    print(f"Loading trained generator weights from: {generator_model_path}")
    print(f"Using Generator architecture with ngf={ngf}")

    try:
        state_dict = torch.load(generator_model_path, map_location=device)

        if next(iter(state_dict)).startswith('module.'):
            print("Removing 'module.' prefix from state_dict keys...")
            new_state_dict = OrderedDict()
            for k, v in state_dict.items():
                name = k[7:]
                new_state_dict[name] = v
            # Load the modified state_dict
            missing_keys, unexpected_keys = netG.load_state_dict(new_state_dict, strict=True) # Use strict=True first
            print(f"Load State Dict Results:")
            print(f"  Missing Keys: {missing_keys}")
            print(f"  Unexpected Keys: {unexpected_keys}")
            if not missing_keys and not unexpected_keys:
                 print("Loaded weights successfully (removed 'module.' prefix).")
            else:
                 print("!!! WARNING: Mismatches detected even after removing 'module.' prefix.")
                 print("Attempting load with strict=False...")
                 netG.load_state_dict(new_state_dict, strict=False)
                 print("Loaded weights with strict=False (some weights might be ignored or extra).")

        else:
            # Load the state_dict directly
            missing_keys, unexpected_keys = netG.load_state_dict(state_dict, strict=True) # Use strict=True first
            print(f"Load State Dict Results:")
            print(f"  Missing Keys: {missing_keys}")
            print(f"  Unexpected Keys: {unexpected_keys}")
            if not missing_keys and not unexpected_keys:
                print("Loaded weights directly successfully.")
            else:
                 print("!!! WARNING: Mismatches detected in state_dict keys.")
                 print("Attempting load with strict=False...")
                 netG.load_state_dict(state_dict, strict=False)
                 print("Loaded weights with strict=False (some weights might be ignored or extra).")


        netG.eval() # Set model to evaluation mode
        print("Generator loaded and set to evaluation mode.")

    except FileNotFoundError:
        print(f"!!! FATAL ERROR: Model file not found at {generator_model_path}. Exiting.")
        exit()
    except Exception as e:
        print(f"!!! FATAL ERROR: An error occurred loading the Generator model: {e}")
        print("Check the Generator class definition and the ngf value against the training script.")
        exit()


    # --- Load Face Detector ---
    try:
        # Use the path provided by cv2.data.haarcascades
        face_cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        if not os.path.exists(face_cascade_path):
             # This case is less likely now with cv2.data.haarcascades, but good practice
             print(f"!!! ERROR: Haar cascade file not found at expected location: {face_cascade_path}")
             # Attempt to download if missing (example - might need pip install requests)
             # import requests
             # url = 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml'
             # print(f"Attempting to download cascade file from {url}")
             # r = requests.get(url, allow_redirects=True)
             # os.makedirs(os.path.dirname(face_cascade_path), exist_ok=True)
             # open(face_cascade_path, 'wb').write(r.content)
             # if not os.path.exists(face_cascade_path):
             #    print("Download failed. Exiting.")
             #    exit()
             # else:
             #    print("Cascade file downloaded.")
             exit() # Exit if still not found
        face_cascade = cv2.CascadeClassifier(face_cascade_path)
        if face_cascade.empty():
             print(f"!!! ERROR: Failed to load Haar cascade from {face_cascade_path}. Check OpenCV installation.")
             exit()
        print("Face detector loaded.")
    except Exception as e:
        print(f"Error loading face detector: {e}")
        exit()

    # --- Capture Photo & Process ---
    print("\nPlease allow webcam access in your browser if prompted.")
    try:
        # Trigger the webcam capture snippet
        captured_filename, frame = take_photo() # frame is now an OpenCV image object
        print(f"Photo captured and saved as {captured_filename}")

        # Display the captured photo using matplotlib
        plt.figure(figsize=(6,6))
        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) # Convert BGR to RGB for plt
        plt.title("Captured Photo")
        plt.axis("off")
        plt.show()

        # --- Emotion Analysis on Captured Frame ---
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        # Adjust detection parameters if needed
        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(40, 40)) # Increased minSize slightly

        detected_emotion_gan_label = None
        if len(faces) == 0:
            print("No face detected in the captured photo.")
        else:
            print(f"{len(faces)} face(s) detected. Analyzing the first detected face.")
            (x, y, w, h) = faces[0] # Process the first detected face

            # Optional: Draw rectangle on the original frame for visualization
            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)
            plt.figure(figsize=(6,6))
            plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            plt.title("Detected Face")
            plt.axis("off")
            plt.show()


            face_roi = frame[y:y+h, x:x+w]

            # Check if ROI is valid
            if face_roi.size == 0:
                 print("Detected face ROI is empty. Skipping analysis.")
            else:
                try:
                    # Use DeepFace for emotion analysis
                    # Consider specifying a backend if default causes issues: backend='opencv', 'ssd', 'dlib', 'mtcnn', 'retinaface'
                    result = DeepFace.analyze(face_roi,
                                              actions=['emotion'],
                                              enforce_detection=False, # Detection already done by Haar
                                              silent=True) # Suppress DeepFace logs unless debugging

                    # DeepFace returns a list containing a dictionary
                    if result and isinstance(result, list) and len(result) > 0:
                        analysis_result = result[0]
                        dominant_emotion = analysis_result.get('dominant_emotion')
                        if dominant_emotion:
                           detected_emotion_gan_label = emotion_map.get(dominant_emotion, 'Neutral/Subdued')
                           print(f"Detected Emotion: {dominant_emotion} -> Mapped to GAN Label: {detected_emotion_gan_label}")
                           # Optional: Print full emotion breakdown
                           # print("Full Emotion Analysis:", analysis_result.get('emotion'))
                        else:
                            print("Dominant emotion not found in DeepFace result.")
                            print("Analysis result:", analysis_result)


                    else:
                        print("Emotion analysis failed or returned empty/unexpected result.")
                        print("DeepFace Result:", result)


                except ValueError as ve:
                     print(f"DeepFace analysis ValueError: {ve}")
                     print("This often happens if the detected face ROI is too small or unclear.")
                except Exception as e:
                    print(f"An unexpected error occurred during DeepFace analysis: {e}")

        # --- Generate Art if Emotion Detected ---
        if detected_emotion_gan_label:
            print(f"\nGenerating art for detected emotion: {detected_emotion_gan_label}")
            generated_tensor = generate_art_from_emotion(detected_emotion_gan_label, netG, device, nz)
            final_image_pil = post_process_image(generated_tensor)

            # Display the generated art
            print(f"Displaying generated art (likely {final_image_pil.width}x{final_image_pil.height} pixels)...")
            plt.figure(figsize=(6,6)) # Keep display size reasonable
            plt.imshow(final_image_pil)
            plt.title(f"Generated Art for: '{detected_emotion_gan_label}'")
            plt.axis("off")
            plt.show()

            # Optionally save the generated art
            try:
                 final_image_pil.save(output_image_file)
                 print(f"Generated art saved as {output_image_file}")
            except Exception as e:
                 print(f"Error saving generated art: {e}")

        else:
            print("\nCould not detect a usable emotion, or no face was found. Skipping art generation.")

    except Exception as err:
      # Catch errors from take_photo (e.g., no webcam, permissions denied) or other processing steps
      print(f"An error occurred during photo capture or processing: {err}")

    print("\n--- Processing finished ---")